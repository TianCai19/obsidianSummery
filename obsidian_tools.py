# File 1: obsidian_tools.py
import os
import re
import signal
import time
from typing import Set
from openai import OpenAI, APIError
from frontmatter import Frontmatter

class SummaryProcessor:
    def __init__(self, client: OpenAI):
        self.client = client
        self.interrupted = False
        self.stats = {
            "total_files": 0,
            "processed_files": 0,
            "skipped_files": 0,
            "total_tokens": 0,
            "start_time": time.time()
        }
        signal.signal(signal.SIGINT, self.handle_interrupt)

    def handle_interrupt(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        self.interrupted = True
        print("\næ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")

    def get_processed_links(self, output_path: str) -> Set[str]:
        """è·å–å·²å¤„ç†çš„æ–‡æ¡£é“¾æ¥"""
        processed = set()
        if os.path.exists(output_path):
            with open(output_path, "r", encoding="utf-8") as f:
                content = f.read()
                matches = re.findall(r"\[\[(.*?)\]\]", content)
                processed = set(matches)
        return processed

    def process_content(self, raw_content: str) -> str:
        """é¢„å¤„ç†Markdownå†…å®¹"""
        try:
            post = Frontmatter.reads(raw_content)
            return post.content
        except:
            return raw_content

    def generate_summary(self, content: str) -> (str, int):
        """ç”Ÿæˆæ‘˜è¦å¹¶è¿”å›tokenä½¿ç”¨é‡"""
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ‘˜è¦åŠ©ç†ï¼Œç”¨3å¥æµç•…çš„ä¸­æ–‡æ€»ç»“ä»¥ä¸‹å†…å®¹çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œä¿æŒä¸“ä¸šä¸”æ˜“äºç†è§£"
                    },
                    {
                        "role": "user", 
                        "content": f"{content[:3500]}"
                    }
                ],
                temperature=0.3,
                max_tokens=256
            )
            
            usage = response.usage.total_tokens
            return response.choices[0].message.content.strip(), usage
            
        except APIError as e:
            print(f"\n[APIé”™è¯¯] çŠ¶æ€ç ï¼š{e.status_code} | é”™è¯¯ä¿¡æ¯ï¼š{e.message}")
        except Exception as e:
            print(f"\n[ç³»ç»Ÿé”™è¯¯] {str(e)}")
        
        return "", 0

    def print_progress(self):
        """æ˜¾ç¤ºå®æ—¶è¿›åº¦"""
        elapsed = time.time() - self.stats["start_time"]
        processed = self.stats["processed_files"]
        total = self.stats["total_files"]
        
        progress = processed / total if total > 0 else 0
        eta = (total - processed) * elapsed / processed if processed > 0 else 0
        
        print(f"\rğŸš€ è¿›åº¦: {processed}/{total} ({progress:.1%}) | "
              f"â³ ç”¨æ—¶: {elapsed:.1f}s | "
              f"ğŸ“Š Token: {self.stats['total_tokens']} | "
              f"ğŸ•’ å‰©ä½™: {eta:.1f}s", end="")

    def init_output_file(self, output_path: str):
        """åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶"""
        if not os.path.exists(output_path):
            with open(output_path, "w", encoding="utf-8") as f:
                f.write("# Obsidian æ–‡æ¡£æ‘˜è¦æ±‡æ€»\n\n")
                f.write("> è‡ªåŠ¨ç”Ÿæˆäº {}\n\n".format(time.strftime("%Y-%m-%d %H:%M")))
        else:
            # æ£€æŸ¥æ–‡ä»¶å¤´æ˜¯å¦å®Œæ•´
            with open(output_path, "r+", encoding="utf-8") as f:
                content = f.read()
                if not content.startswith("# Obsidian æ–‡æ¡£æ‘˜è¦æ±‡æ€»"):
                    f.seek(0, 0)
                    f.write("# Obsidian æ–‡æ¡£æ‘˜è¦æ±‡æ€»\n\n> è‡ªåŠ¨ç”Ÿæˆäº {}\n\n".format(time.strftime("%Y-%m-%d %H:%M")) + content)

    def _walk_vault(self, vault_path: str):
        """ç”Ÿæˆå™¨ï¼šé¢„éå†çŸ¥è¯†åº“æ–‡ä»¶"""
        for root, _, files in os.walk(vault_path):
            if self.interrupted:
                return
            if any(x in root for x in ["templates", ".trash"]):
                continue
            for file in files:
                if file.endswith(".md"):
                    yield os.path.join(root, file)

    def _should_process(self, file_path: str, processed_links: set) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦éœ€è¦å¤„ç†"""
        rel_path = os.path.relpath(file_path, start=self.vault_path)
        obsidian_link = rel_path.replace(".md", "").replace("\\", "/")
        return (
            os.path.getsize(file_path) > 1024 and  # è¿‡æ»¤å°æ–‡ä»¶
            obsidian_link not in processed_links and
            not any(p in rel_path for p in ["_templates", "Daily Notes"])  # é¢å¤–è¿‡æ»¤æ¡ä»¶
        )

    def process_single_file(self, file_path: str, output_path: str, lock):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                raw_content = f.read()
                
            if not raw_content.strip():
                return 0

            content = self.process_content(raw_content)
            summary, tokens = self.generate_summary(content)
            
            if summary:
                rel_path = os.path.relpath(file_path, start=self.vault_path)
                obsidian_link = rel_path.replace(".md", "").replace("\\", "/")
                entry = f"- [[{obsidian_link}]]\n  {summary}\n\n"
                
                # ä½¿ç”¨çº¿ç¨‹é”ä¿è¯å†™å…¥å®‰å…¨
                with lock:
                    with open(output_path, "a", encoding="utf-8") as f:
                        f.write(entry)

            return tokens
            
        except Exception as e:
            print(f"\nå¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return 0

    def process_vault(self, vault_path: str, output_file: str) -> None:
        """å¤„ç†æ•´ä¸ªçŸ¥è¯†åº“ï¼ˆå¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆï¼‰"""
        self.vault_path = vault_path
        output_path = os.path.join(vault_path, output_file)
        processed_links = self.get_processed_links(output_path)
        self.init_output_file(output_path)

        # é¢„æ‰«æå¹¶è¿‡æ»¤æ–‡ä»¶
        file_list = []
        for file_path in self._walk_vault(vault_path):
            if self._should_process(file_path, processed_links):
                file_list.append(file_path)
        self.stats["total_files"] = len(file_list)
        self.stats["skipped_files"] = sum(1 for _ in self._walk_vault(vault_path)) - len(file_list)

        try:
            from concurrent.futures import ThreadPoolExecutor, as_completed
            import threading
            
            # åˆå§‹åŒ–çº¿ç¨‹é”
            write_lock = threading.Lock()
            # åŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°ï¼ˆ4-8ä¸ªï¼‰
            max_workers = min(8, max(4, (os.cpu_count() or 4)))
            completed = 0
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(self.process_single_file, f, output_path, write_lock): f for f in file_list}
                
                for future in as_completed(futures):
                    if self.interrupted:
                        executor.shutdown(wait=False, cancel_futures=True)
                        break
                        
                    tokens = future.result()
                    self.stats["processed_files"] += 1
                    self.stats["total_tokens"] += tokens
                    completed += 1
                    
                    # é™ä½è¿›åº¦åˆ·æ–°é¢‘ç‡ï¼ˆæ¯å¤„ç†1%æˆ–è‡³å°‘1ç§’ï¼‰
                    if completed % max(1, len(file_list)//100) == 0 or time.time() - self.stats["start_time"] > 1:
                        self.print_progress()
            
            print(f"\n\nâœ… å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")
            
        finally:
            total_time = time.time() - self.stats["start_time"]
            print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ï¼š")
            print(f"- æ€»æ–‡ä»¶æ•°: {self.stats['total_files'] + self.stats['skipped_files']}")
            print(f"- å¾…å¤„ç†æ–‡ä»¶: {self.stats['total_files']}")
            print(f"- æˆåŠŸå¤„ç†: {self.stats['processed_files']}")
            print(f"- è·³è¿‡æ–‡ä»¶: {self.stats['skipped_files']}")
            print(f"- æ€»Tokenç”¨é‡: {self.stats['total_tokens']}")
            print(f"- æ€»è€—æ—¶: {total_time:.1f}ç§’")
            if total_time > 0:
                print(f"- å¤„ç†é€Ÿåº¦: {self.stats['processed_files']/total_time:.1f} æ–‡ä»¶/ç§’")
                print(f"- Tokené€Ÿç‡: {self.stats['total_tokens']/total_time:.1f} token/ç§’")
